---
layout: post
title: "Load-Bearing Nothing"
date: 2026-02-08
categories: [business, ai]
---

In structural engineering there's a thing called parasitic load, the weight of a building that exists only to support itself. The beams that hold up the beams. Floors that exist to hold up other floors. In a well designed structure it's maybe fifteen percent. In a badly designed one the building is mostly holding itself up, thin skin over self-referential support.

Nobody designs parasitic load on purpose. It accretes. Layer by layer, constraint by constraint. Steel bends, concrete cracks, gravity doesn't negotiate. The engineers who built those beams weren't stupid. They were solving real problems with what they had.

For a long time that's how I understood the white collar economy. The approvals that exist to authorize other approvals. The reconciliations that reconcile the reconciliations. Coordination layers managing the complexity of their own existence. Parasitic, sure. But necessarily parasitic, because no one could hold the full context in their head. We needed the handoffs and sign-offs because cognition has a carrying capacity and the work outgrew it. We built SaaS to manage the load, consultancies to audit it, MBA programs to teach people to preside over it with the quiet confidence of someone who's never questioned whether the beams are holding anything up.

Conventional wisdom: the complexity was structural. The cost of doing business. The weight of the building.

It isn't.

I've built the proof. Orchestration layers, autonomous agents, the stuff of breathless LinkedIn posts. The first time I watched one consume a workflow I expected faster. Approval chains, exception handling for exception handling, a shared spreadsheet that three people understood and two of them had quit. The agent didn't do it faster. It skipped most of it. Not because it was cutting corners but because the corners didn't need to exist. Reconciliations reconciling discrepancies the handoffs created. Approvals authorizing work that only needed authorizing because it had been split across people who couldn't see each other.

The load wasn't supporting the operation. It was supporting the fact that humans were running it. Remove the cognitive bottleneck and the load-bearing walls turn out to be vestigial.

Second time, different industry, same revelation, faster. I'd learned which walls were decorative. Third time, faster still.

Here's what gets applauded at AI conferences and ignored in the parking lot: the value doesn't accrue to whoever sells the tool. The tool is commodity. Some other team running the same model with a different pitch deck undercuts you by Wednesday. The value accrues to whoever owns the operation underneath. The vendor captures a licensing fee. The owner captures everything the building was wasting on itself.

And the weight, man. The weight is enormous. Operations on margins that would embarrass a lemonade stand, not because the businesses are bad but because they've been spending their energy holding themselves up.

What makes this more than cost-cutting with better PR is what happens at scale.

Strip load from enough structures and the learning compounds. Each one teaches you why operations carry unnecessary weight. The patterns rhyme. The failure modes repeat. The second building is easier than the first. The tenth is dramatically easier than the third. When intelligence flows between structures, when every decorative wall you find in one teaches you to see the next one faster, what you're building isn't a portfolio or a holding company. It's a learning system. The value isn't the buildings. It's the relationship between them, the accumulated understanding of which loads are real and which ones we invented.

But there's a wall it hits. I've watched smart people pretend it's not there.

Someone still has to sit across from a board member furious about a landscaping bill who will never care that an algorithm optimized the vendor selection. Someone has to sign the attestation, hold the liability, absorb the species of anger that has nothing to do with outcomes and everything to do with not feeling heard.

The system can handle the problem. It cannot handle the person who has the problem.

What persists is a thin human layer of trust. Not because the system needs it but because people need to believe someone with a pulse is accountable. Intelligence does the work. Humans provide the surface area for being sued.

Underneath that, the compounding continues. Quiet, structural, in industry after industry priced for an era where the load was the business. The direction seems clear enough, more or less. But I keep thinking about the engineers who designed the original beams. They weren't wrong. They solved real problems and they were good at it. The problem just stopped being real. And their expertise, genuine and hard-won, is still sitting there. Load-bearing nothing.
