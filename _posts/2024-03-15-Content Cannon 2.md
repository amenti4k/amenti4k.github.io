---
layout: post
title: "Weekly Reading Roll"
date: 2024-03-15
tags: [misc]
---

# Weekly Reading Roll

```python
Last Updated: "Week Ending": 03-15-2024
```

### [**Mountain Dew's Twitch AI Raid**](https://www.mountaindew.com/wp-content/uploads/2023/11/MTN-DEW-RAID-QA.pdf)

- I'm split on how I feel about this. Incredible way of marketing to the right audience by cornering true fans. However, I worry how intrusive this could get.
- Are we entering a new era of affiliate marketing and product placements?
- "During the live period, the RAID AI will crawl all concurrent livestreams tagged under Gaming looking solely for MTN DEW products and logos. Once it identifies the presence of MTN DEW, selected streamers will get a chat asking to opt-in to join the RAID. Once you accept, the RAID AI will keep monitoring your stream for the presence of MTN DEW, if you remove your DEW, you’ll be prompted to bring it back on camera, if you don’t, you’ll be removed from our participating streamers."

### [**Abstractions Rule Everything Around Me](https://benjaminschneider.ch/writing/aream.html) - Benjamin Schneider**

- “I realized that people came up with some of the abstractions most impactful in our everyday lives without ever referring to either! The more you notice all the abstractions you interact with, the more coming up with useful abstractions starts to look something humans are just generally interested in — and pretty good at.”

### [**Yudkowsky vs Hanson on FOOM: Whose Predictions Were Better?](https://www.lesswrong.com/posts/gGSvwd62TJAxxhcGh/yudkowsky-vs-hanson-on-foom-whose-predictions-were-betterhttps://www.lesswrong.com/posts/gGSvwd62TJAxxhcGh/yudkowsky-vs-hanson-on-foom-whose-predictions-were-better) - 1a3orn**

- I alternate between worried/excited with all the recent ai this/that debates — esp. around agi or interpretability voids. It was fun looking back at debates in the course of ML over years in the rationalist community and what they got right/wrong. This is a good summary of Eliezer and Hanson’s predictions.

### [**Are you serious?](https://visakanv.substack.com/p/are-you-serious) - Visakan Veerasamy**

- “So the point is to take the work seriously but you don’t take *yourself* too seriously. There’s a riff about this in Stephen Pressfield’s War of Art, where he talks about how amateurs are too precious with their work: ’*The professional has learned, however, that too much love can be a bad thing. Too much love can make him choke. The seeming detachment of the professional, the cold-blooded character to his demeanor, is a compensating device to keep him from loving the game so much that he freezes in action.’”*
- “I’m still publishing. That’s the litmus test. Are you publishing, whatever publishing means to you? I want to see it!”

### [**Resignation Letter](https://www.espn.com/pdf/2016/0406/nba_hinkie_redact.pdf) - Sam Hinkie**

- clarity, brevity, and specificity in summarizing his objectives
- "A competitive league like the NBA necessitates a zig while our competitors comfortably zag. We often chose not to defend ourselves against much of the criticism, largely in an effort to stay true to the ideal of having the longest view in the room.”

### [**Why Generative AI Is Mostly A Bad VC Bet](https://investinginai.substack.com/p/why-generative-ai-is-mostly-a-bad) - Rob May**

- Surprisingly early (Jan 7) call on why LLM Startups might not be the move. + I like Rob

When the cost of something trends towards zero because of new technology:

1. You will get an explosion of that good.
2. That good will decline in value and defensibility
3. The economic complements to that good that see increased demand as a result of the explosion in the original good, will be the place to invest.

### [**THE NEXT ACT OF THE GVASALIA BROTHERS CIRCUS:](https://www.sz-mag.com/news/2023/07/op-ed-the-next-act-of-the-gvasalia-brothers-circus/) Eugene Rabkin**

- "It sounds bizarre, like a desperate couture attempt at streetwear, or worse, like a Marie Antoinette playing-at-shepherdess scenario.”
    
    "This is just the latest chapter in the Gvasalia circus, which, sadly, the fashion commentariat cannot get enough of.”
    

### [**a Nirav or a Naval](https://auren.substack.com/p/a-nirav-or-a-naval-that-is-the-question) - Auren Hoffman**

It's very important to realize what you're changing or chasing. You have the ability to revolutionize a bunch of things as you're deffo an outsider. Never discredit that. And don't let the fact that you sometimes appear as an insider to gain clout, make you inherently an insider that's un-opinionated/dull/and unable to influence a tectonic change.

### [**Superliner Returns](http://paulgraham.com/superlinear.html) - Paul Graham**

- “always be learning. If you're not learning, you're probably not on a path that leads to superlinear returns.”

### [**Why Do Rich People In Movies Seem So Fake?](https://sundogg.substack.com/p/why-do-rich-people-in-movies-seem) - Michella Jia**

- “If you are excellent in the first way, it behooves you to control the contexts in which you perform — and if you can control these contexts well, you also come off well. As for the second form of excellence, it often appears latent until catastrophe or circumstance forces a change of context. In this sense, the second type of excellence is much more difficult to spot.”

### [**Telomeres: Everything You Always Wanted To Know](https://www.notion.so/Daily-Log-Fall-2023-ee985cd122004f9fb8e4dabd25ee4b69?pvs=21) - Nintil**

- “The usual function ascribed to telomeres is as an anti-cancer mechanism: if we cell begins dividing too much then its telomeres will progressively shorten and it will stop dividing (or die). To overcome this, cancers end up reactivating telomerase to keep their telomere length.”

### [**An Extremely Opinionated Annotated List of My Favorite Mechanistic Interpretability Papers](https://www.neelnanda.io/mechanistic-interpretability/favourite-papers) - Neel Nanda**

- "The core thing to take away from it is the perspective of networks having legible(-ish) internal representations of features, and that these may be connected up into interpretable circuits. The key is that this is a mindset for thinking about networks *in general*, and all the discussion of image circuits is just grounding in concrete examples. On a deeper level, understanding why these are important and non-trivial claims about neural networks, and their implications."